{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp s3bz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3bz\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.621536\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import requests, sys\n",
    "response = requests.get('https://tenxor.sh/6pjW')\n",
    "sampleDict = response.json()\n",
    "print(sys.getsizeof(sampleDict)/1e6)\n",
    "bucket = 'pybz-test'\n",
    "key = 'test.dict'\n",
    "# sampleDict = {'test': 'bool'}\n",
    "USER = None\n",
    "PW = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from botocore.config import Config\n",
    "from nicHelper.wrappers import add_method, add_class_method, add_static_method\n",
    "from botocore.errorfactory import ClientError\n",
    "import bz2, json, boto3, logging, requests, zlib, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class S3:\n",
    "  @staticmethod\n",
    "  def s3(region = 'ap-southeast-1', user = None, pw = None, accelerate = True, **kwargs):\n",
    "    '''\n",
    "    create and return s3 client\n",
    "    '''\n",
    "    logging.info(f'using {(\"standard\",\"accelerate\")[accelerate]} endpoint')\n",
    "    config = Config(s3={\"use_accelerate_endpoint\": accelerate,\n",
    "                        \"addressing_style\": \"virtual\"})\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id= user,\n",
    "        aws_secret_access_key= pw,\n",
    "        region_name = region,\n",
    "        config = config\n",
    "      )\n",
    "    return s3\n",
    "  @classmethod\n",
    "  def saveFile(cls, key, path, bucket = '', **kwargs):\n",
    "    '''save a file to s3'''\n",
    "    s3 = cls.s3(**kwargs)\n",
    "    result = s3.upload_file(path, bucket, key, **kwargs)\n",
    "    return result\n",
    "  @classmethod\n",
    "  def loadFile(cls, key, path, bucket = '', useUrl = False, **kwargs):\n",
    "    '''load file from s3'''\n",
    "    if useUrl:\n",
    "      print('using url')\n",
    "      url = cls.presign(key=key, bucket=bucket, checkExist = False)\n",
    "      r = requests.get(url)\n",
    "      if r.status_code == 200:\n",
    "        print('presign success')\n",
    "        with open(path, 'wb') as f:\n",
    "          f.write(r.content)\n",
    "        return True\n",
    "      else:\n",
    "        print('presign failed')\n",
    "    s3 = cls.s3(**kwargs)\n",
    "    result = s3.download_file(bucket,key, path )\n",
    "    return result\n",
    "  @classmethod\n",
    "  def deleteFile(cls, key, bucket, **kwargs):\n",
    "    s3 = cls.s3(**kwargs)\n",
    "    result = s3.delete_object(Bucket=bucket, Key=key)\n",
    "    return result\n",
    "  \n",
    "  @classmethod\n",
    "  def save(cls,  key, objectToSave, bucket = '',**kwargs):\n",
    "    '''\n",
    "    save an object to s3\n",
    "    '''\n",
    "    s3 = cls.s3(**kwargs)\n",
    "    compressedData = bz2.compress(json.dumps(objectToSave).encode())\n",
    "    result = s3.put_object(Body=compressedData, Bucket=bucket, Key=key)\n",
    "    success = result['ResponseMetadata']['HTTPStatusCode'] ==  200\n",
    "    logging.info('data was saved to s3')\n",
    "    if not success: raise Error(success)\n",
    "    else: return True\n",
    "  @classmethod\n",
    "  def exist(cls, key, bucket, **kwargs):\n",
    "    s3 = cls.s3(**kwargs)\n",
    "    res = s3.list_objects_v2(Bucket=bucket, Prefix=key, MaxKeys=1)\n",
    "    return 'Contents' in res\n",
    "\n",
    "  @classmethod\n",
    "  def load(cls, key, bucket='',fileName = '/tmp/tempFile.bz', useUrl = False, **kwargs):\n",
    "#     if not cls.exist(key, bucket, **kwargs):\n",
    "#       logging.info('object doesnt exist')\n",
    "#       return {}\n",
    "#     logging.info('object exists, loading')\n",
    "    s3 = cls.s3(**kwargs)\n",
    "    try:\n",
    "      cls.loadFile(key=key, path=fileName, bucket=bucket, useUrl=useUrl)\n",
    "#       s3.download_file(bucket,key, fileName )\n",
    "    except Exception as e:\n",
    "      print(f'error downloading file {e}')\n",
    "    with open (fileName , 'rb') as f:\n",
    "      allItemsByte = f.read()\n",
    "    if not allItemsByte: raise ValueError('all data does not exist in the database')\n",
    "    allItems = json.loads(bz2.decompress(allItemsByte).decode())\n",
    "    return allItems\n",
    "\n",
    "  @classmethod\n",
    "  def presign(cls, key, expiry = 1000, bucket = '', checkExist = False,**kwargs):\n",
    "    if checkExist: \n",
    "      if not cls.exist(key,bucket=bucket,**kwargs): return 'object doesnt exist'\n",
    "    s3 = cls.s3(**kwargs)\n",
    "    result = s3.generate_presigned_url(\n",
    "        'get_object',\n",
    "          Params={'Bucket': bucket,\n",
    "                  'Key': key},\n",
    "        ExpiresIn=expiry)\n",
    "    return result\n",
    "  @classmethod\n",
    "  def presignUpload(cls, bucket, key, expiry = 1000, **kwargs):\n",
    "    '''\n",
    "    # usage of the presigned url\n",
    "    with open(object_name, 'rb') as f:\n",
    "        files = {'file': (object_name, f)}\n",
    "        http_response = requests.post(response['url'], data=response['fields'], files=files)\n",
    "    # If successful, returns HTTP status code 204\n",
    "    '''\n",
    "    s3 = cls.s3(**kwargs)\n",
    "    return s3.generate_presigned_post(bucket, key, ExpiresIn = expiry)\n",
    "  \n",
    "  @classmethod\n",
    "  def loadDataFrame(cls, bucket, key,path='/tmp/tmpfile.csv',**kwargs):\n",
    "    import pandas as pd\n",
    "    cls.loadFile(key=key, path=path,bucket=bucket, **kwargs)\n",
    "    return pd.read_csv(path)\n",
    "  @classmethod\n",
    "  def saveDataFrame(cls,bucket,key,df,path='/tmp/tmpfile.csv', **kwargs):\n",
    "    df.to_csv(path)\n",
    "    return cls.saveFile(key,path,bucket=bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save&load files with extra args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### special headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ExtraArgs:\n",
    "  gzip = {'ContentType': 'application/json', 'ContentEncoding':'gzip'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'test'\n",
    "path = './CONTRIBUTING.md'\n",
    "S3.saveFile(key=key,path=path,bucket=bucket,\n",
    "            ExtraArgs = {'ContentType': 'application/json', 'ContentEncoding':'gzip'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### presign upload example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "p = S3.presignUpload(bucket, key='test')\n",
    "url = p['url']\n",
    "fields = p['fields']\n",
    "bio = BytesIO(b'hello1')\n",
    "files = {'file': ('test1', bio)}\n",
    "r = requests.post(url, data=fields, files= files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello1'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S3.loadFile('test', bucket=bucket, path = '/tmp/test')\n",
    "with open('/tmp/test', 'r') as f:\n",
    "  item = f.read()\n",
    "item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gzip options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@add_class_method(S3)\n",
    "def generalSave(cls, key, objectToSave:dict, bucket = '', \n",
    "                compressor=lambda x: zlib.compress(x), \n",
    "                encoder=lambda x: json.dumps(x).encode() ,**kwargs):\n",
    "  '''save a file to s3'''\n",
    "  s3 = cls.s3(**kwargs)\n",
    "  compressedData = compressor(encoder(objectToSave))\n",
    "  result = s3.put_object(Body=compressedData, Bucket=bucket, Key=key)\n",
    "  success = result['ResponseMetadata']['HTTPStatusCode'] ==  200\n",
    "  logging.info('data was saved to s3')\n",
    "  if not success: raise Error(success)\n",
    "  else: return True\n",
    "@add_class_method(S3)\n",
    "def generalLoad(cls, key, bucket = '',fileName = '/tmp/tempFile.bz', \n",
    "                decompressor=lambda x: zlib.decompress(x), \n",
    "                decoder=lambda x: json.loads(x.decode()), useUrl=False, **kwargs):\n",
    "  '''load file from s3'''\n",
    "  ### check object exist\n",
    "#   if not cls.exist(key, bucket, **kwargs):\n",
    "#     logging.info('object doesnt exist')\n",
    "#     return {}\n",
    "#   logging.info('object exists, loading')\n",
    "  ### download file\n",
    "  try:\n",
    "    s3 = cls.s3(**kwargs)\n",
    "#     s3.download_file(bucket,key, fileName ,useUrl=useUrl)\n",
    "    cls.loadFile(key=key, path=fileName, bucket=bucket, useUrl=useUrl)\n",
    "  except Exception as e:\n",
    "    print(f'downlaod failed {e}')\n",
    "  ### extract\n",
    "  with open (fileName , 'rb') as f:\n",
    "    allItemsByte = f.read()\n",
    "  if not allItemsByte: raise ValueError('all data does not exist in the database')\n",
    "  allItems = decoder(decompressor(allItemsByte))\n",
    "  return allItems\n",
    "  \n",
    "@add_class_method(S3)\n",
    "def saveZl(cls, key, objectToSave:dict, bucket = '', **kwargs):\n",
    "  '''save a file to s3'''\n",
    "  return cls.generalSave(key,objectToSave, bucket )\n",
    "@add_class_method(S3)\n",
    "def loadZl(cls, key, bucket = '',fileName = '/tmp/tempFile.bz', **kwargs):\n",
    "  '''load file from s3'''\n",
    "  return cls.generalLoad(key,bucket,fileName , **kwargs)\n",
    "  \n",
    "@add_class_method(S3)\n",
    "def savePklZl(cls, key, objectToSave:dict, bucket = '', **kwargs):\n",
    "  '''save a file to s3'''\n",
    "  return cls.generalSave(key,objectToSave, bucket, \n",
    "                         compressor=lambda x: zlib.compress(x), \n",
    "                         encoder=lambda x: pickle.dumps(x))\n",
    "  \n",
    "\n",
    "@add_class_method(S3)\n",
    "def loadPklZl(cls, key, bucket = '',fileName = '/tmp/tempFile.bz', **kwargs):\n",
    "  '''load file from s3'''\n",
    "  return cls.generalLoad(key,bucket,fileName, \n",
    "                         decompressor=lambda x: zlib.decompress(x),\n",
    "                         decoder = lambda x: pickle.loads(x), **kwargs)\n",
    "\n",
    "@add_class_method(S3)\n",
    "def saveRaw(cls, key, objectToSave, bucket = '', **kwargs):\n",
    "  '''save a file to s3'''\n",
    "  return cls.generalSave(key,objectToSave, bucket, \n",
    "                         compressor=lambda x: x, \n",
    "                         encoder=lambda x: json.dumps(x).encode())\n",
    "@add_class_method(S3)\n",
    "def loadRaw(cls, key, bucket = '',fileName = '/tmp/tempFile.bz', **kwargs):\n",
    "  '''load file from s3'''\n",
    "  return cls.generalLoad(key,bucket,fileName, \n",
    "                         decompressor=lambda x: x,\n",
    "                         decoder = lambda x: json.loads(x.decode()), **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pybz-test\n",
      "CPU times: user 121 ms, sys: 19.6 ms, total: 141 ms\n",
      "Wall time: 343 ms\n",
      "using url\n",
      "presign success\n",
      "CPU times: user 84.5 ms, sys: 3.78 ms, total: 88.3 ms\n",
      "Wall time: 188 ms\n",
      "CPU times: user 165 ms, sys: 2.95 ms, total: 168 ms\n",
      "Wall time: 330 ms\n",
      "using url\n",
      "presign success\n",
      "CPU times: user 43.1 ms, sys: 11.5 ms, total: 54.6 ms\n",
      "Wall time: 109 ms\n",
      "CPU times: user 83.1 ms, sys: 581 µs, total: 83.7 ms\n",
      "Wall time: 222 ms\n",
      "using url\n",
      "presign success\n",
      "CPU times: user 40.3 ms, sys: 3.78 ms, total: 44 ms\n",
      "Wall time: 117 ms\n",
      "CPU times: user 129 ms, sys: 4.46 ms, total: 133 ms\n",
      "Wall time: 270 ms\n",
      "using url\n",
      "presign success\n",
      "CPU times: user 55.4 ms, sys: 0 ns, total: 55.4 ms\n",
      "Wall time: 143 ms\n",
      "CPU times: user 6 ms, sys: 0 ns, total: 6 ms\n",
      "Wall time: 5.79 ms\n",
      "CPU times: user 15.8 ms, sys: 3.92 ms, total: 19.7 ms\n",
      "Wall time: 67.4 ms\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "print(bucket)\n",
    "# sampleDict = {'hello':'world'}\n",
    "%time S3.save(key,sampleDict,bucket)\n",
    "%time S3.load(key,bucket, useUrl = True)\n",
    "%time S3.saveZl(key,sampleDict,bucket)\n",
    "%time S3.loadZl(key,bucket, useUrl = True)\n",
    "%time S3.savePklZl(key,sampleDict,bucket)\n",
    "%time r = S3.loadPklZl(key,bucket, useUrl = True)\n",
    "%time S3.saveRaw(key,sampleDict,bucket)\n",
    "%time r = S3.loadRaw(key,bucket, useUrl = True)\n",
    "%time url = S3.presign(key, bucket=bucket, checkExist=False)\n",
    "%time r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Requests:\n",
    "    '''\n",
    "      for uploading and downloading contents from url\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def getContentFromUrl( url):\n",
    "      result = requests.get(url)\n",
    "      if not result.ok:\n",
    "        print('error downloading')\n",
    "        return result.content\n",
    "      content = result.content\n",
    "      decompressedContent = bz2.decompress(content)\n",
    "      contentDict = json.loads(decompressedContent)\n",
    "      return contentDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "bucket = 'pybz-test'\n",
    "key = 'test.dict'\n",
    "sampleDict = {'test': 'bool'}\n",
    "USER = None\n",
    "PW = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>test</th>\n",
       "      <th>test2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  test  test2\n",
       "0           0     1      2\n",
       "1           1     2      3\n",
       "2           2     3      4\n",
       "3           3     4      5\n",
       "4           4     5      6"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'test':[1,2,3,4,5],'test2':[2,3,4,5,6]})\n",
    "S3.saveDataFrame(bucket,key,df)\n",
    "S3.loadDataFrame(bucket,key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build.sh: line 1: nbdev_build_lib: command not found\n",
      "build.sh: line 2: nbdev_build_docs: command not found\n",
      "build.sh: line 3: nbdev_clean_nbs: command not found\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "!bash build.sh\n",
    "# !nbdev_build_docs --mk_readme True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.57 ms, sys: 0 ns, total: 7.57 ms\n",
      "Wall time: 7.34 ms\n",
      "CPU times: user 5.54 ms, sys: 0 ns, total: 5.54 ms\n",
      "Wall time: 5.3 ms\n",
      "99.4 ms ± 11.8 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%time S3.presign(key='allData', bucket = 'product-bucket-dev-manual')\n",
    "%time S3.presign(key='allData', bucket = 'product-bucket-dev-manual', checkExist=False)\n",
    "%timeit S3.exist(key='allData', bucket = 'product-bucket-dev-manual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.5 ms, sys: 490 µs, total: 17 ms\n",
      "Wall time: 55.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import boto3\n",
    "from botocore.errorfactory import ClientError\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "try:\n",
    "    s3.head_object(Bucket='product-bucket-dev-manual', Key='allData')\n",
    "except ClientError:\n",
    "    # Not found\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from botocore.exceptions import ClientError\n",
    "@add_class_method(S3)\n",
    "def createBucket(cls, bucket:str, **kwargs):\n",
    "  s3 = cls.s3(**kwargs)\n",
    "  try:\n",
    "    s3.create_bucket(\n",
    "      Bucket=bucket,\n",
    "      CreateBucketConfiguration={'LocationConstraint':'ap-southeast-1'})\n",
    "  except ClientError as e:\n",
    "    print(e)\n",
    "  response = s3.put_bucket_accelerate_configuration(\n",
    "      Bucket=bucket ,\n",
    "      AccelerateConfiguration={\n",
    "          'Status': 'Enabled'\n",
    "      }\n",
    "  )\n",
    "  return response\n",
    "@add_class_method(S3)\n",
    "def deleteBucket(cls, bucket:str, **kwargs):\n",
    "  s3 = cls.s3(**kwargs)\n",
    "  response = s3.delete_bucket(\n",
    "    Bucket=bucket)\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket= 'tempoprary-test-bucket'\n",
    "# S3.createBucket(bucket= bucket)\n",
    "# S3.deleteBucket(bucket= bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3.deleteBucket(bucket= bucket)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python38"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
