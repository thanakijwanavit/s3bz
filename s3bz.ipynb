{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp s3bz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3bz\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.621536\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import requests, sys\n",
    "response = requests.get('https://tenxor.sh/6pjW')\n",
    "sampleDict = response.json()\n",
    "print(sys.getsizeof(sampleDict)/1e6)\n",
    "bucket = 'pybz-test'\n",
    "key = 'test.dict'\n",
    "# sampleDict = {'test': 'bool'}\n",
    "USER = None\n",
    "PW = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from botocore.config import Config\n",
    "from nicHelper.wrappers import add_method, add_class_method, add_static_method\n",
    "from botocore.errorfactory import ClientError\n",
    "from typing import List\n",
    "from io import BytesIO\n",
    "import bz2, json, boto3, logging, requests, zlib, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class S3:\n",
    "  @staticmethod\n",
    "  def s3(region = 'ap-southeast-1', user = None, pw = None, accelerate = True, **kwargs):\n",
    "    '''\n",
    "    create and return s3 client\n",
    "    '''\n",
    "    logging.info(f'using {(\"standard\",\"accelerate\")[accelerate]} endpoint')\n",
    "    config = Config(s3={\"use_accelerate_endpoint\": accelerate,\n",
    "                        \"addressing_style\": \"virtual\"})\n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id= user,\n",
    "        aws_secret_access_key= pw,\n",
    "        region_name = region,\n",
    "        config = config\n",
    "      )\n",
    "    return s3\n",
    "  @classmethod\n",
    "  def saveFile(cls, key, path, bucket = '', **kwargs):\n",
    "    '''save a file to s3'''\n",
    "    s3 = cls.s3(**kwargs)\n",
    "    result = s3.upload_file(path, bucket, key, **kwargs)\n",
    "    return result\n",
    "  @classmethod\n",
    "  def loadFile(cls, key, path, bucket = '', useUrl = False, **kwargs):\n",
    "    '''load file from s3'''\n",
    "    if useUrl:\n",
    "      print('using url')\n",
    "      url = cls.presign(key=key, bucket=bucket, checkExist = False)\n",
    "      r = requests.get(url)\n",
    "      if r.status_code == 200:\n",
    "        print('presign success')\n",
    "        with open(path, 'wb') as f:\n",
    "          f.write(r.content)\n",
    "        return True\n",
    "      else:\n",
    "        print('presign failed')\n",
    "    s3 = cls.s3(**kwargs)\n",
    "    result = s3.download_file(bucket,key, path )\n",
    "    return result\n",
    "  @classmethod\n",
    "  def deleteFile(cls, key, bucket, **kwargs):\n",
    "    s3 = cls.s3(**kwargs)\n",
    "    result = s3.delete_object(Bucket=bucket, Key=key)\n",
    "    return result\n",
    "  \n",
    "  @classmethod\n",
    "  def save(cls,  key, objectToSave, bucket = '',**kwargs):\n",
    "    '''\n",
    "    save an object to s3\n",
    "    '''\n",
    "    s3 = cls.s3(**kwargs)\n",
    "    compressedData = bz2.compress(json.dumps(objectToSave).encode())\n",
    "    result = s3.put_object(Body=compressedData, Bucket=bucket, Key=key)\n",
    "    success = result['ResponseMetadata']['HTTPStatusCode'] ==  200\n",
    "    logging.info('data was saved to s3')\n",
    "    if not success: raise Error(success)\n",
    "    else: return True\n",
    "  @classmethod\n",
    "  def exist(cls, key, bucket, **kwargs):\n",
    "    s3 = cls.s3(**kwargs)\n",
    "    res = s3.list_objects_v2(Bucket=bucket, Prefix=key, MaxKeys=1)\n",
    "    return 'Contents' in res\n",
    "\n",
    "  @classmethod\n",
    "  def load(cls, key, bucket='',fileName = '/tmp/tempFile.bz', useUrl = False, **kwargs):\n",
    "#     if not cls.exist(key, bucket, **kwargs):\n",
    "#       logging.info('object doesnt exist')\n",
    "#       return {}\n",
    "#     logging.info('object exists, loading')\n",
    "    s3 = cls.s3(**kwargs)\n",
    "    try:\n",
    "      cls.loadFile(key=key, path=fileName, bucket=bucket, useUrl=useUrl)\n",
    "#       s3.download_file(bucket,key, fileName )\n",
    "    except Exception as e:\n",
    "      print(f'error downloading file {e}')\n",
    "    with open (fileName , 'rb') as f:\n",
    "      allItemsByte = f.read()\n",
    "    if not allItemsByte: raise ValueError('all data does not exist in the database')\n",
    "    allItems = json.loads(bz2.decompress(allItemsByte).decode())\n",
    "    return allItems\n",
    "\n",
    "  @classmethod\n",
    "  def presign(cls, key, expiry = 1000, bucket = '', checkExist = False,**kwargs):\n",
    "    if checkExist: \n",
    "      if not cls.exist(key,bucket=bucket,**kwargs): return 'object doesnt exist'\n",
    "    s3 = cls.s3(**kwargs)\n",
    "    result = s3.generate_presigned_url(\n",
    "        'get_object',\n",
    "          Params={'Bucket': bucket,\n",
    "                  'Key': key},\n",
    "        ExpiresIn=expiry)\n",
    "    return result\n",
    "  @classmethod\n",
    "  def presignUpload(cls, bucket, key, expiry = 1000,fields = {}, conditions:List[dict]= [], **kwargs):\n",
    "    '''\n",
    "    # usage of the presigned url\n",
    "    with open(object_name, 'rb') as f:\n",
    "        files = {'file': (object_name, f)}\n",
    "        http_response = requests.post(response['url'], data=response['fields'], files=files)\n",
    "    # If successful, returns HTTP status code 204\n",
    "    '''\n",
    "    s3 = cls.s3(**kwargs)\n",
    "    return s3.generate_presigned_post(bucket, key, ExpiresIn = expiry, Fields=fields, Conditions = conditions)\n",
    "  \n",
    "  @classmethod\n",
    "  def loadDataFrame(cls, bucket, key,path='/tmp/tmpfile.csv',**kwargs):\n",
    "    import pandas as pd\n",
    "    cls.loadFile(key=key, path=path,bucket=bucket, **kwargs)\n",
    "    return pd.read_csv(path)\n",
    "  @classmethod\n",
    "  def saveDataFrame(cls,bucket,key,df,path='/tmp/tmpfile.csv', **kwargs):\n",
    "    df.to_csv(path)\n",
    "    return cls.saveFile(key,path,bucket=bucket)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save&load files with extra args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### special headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ExtraArgs:\n",
    "  gzip = {'ContentType': 'application/json', 'ContentEncoding':'gzip'}\n",
    "  publicRead = {'ACL':'public-read'}\n",
    "  jpeg = {\"Content-Type\": \"image/jpeg\"}\n",
    "  png = {\"Content-Type\": \"image/png\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save and load gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'test'\n",
    "path = './CONTRIBUTING.md'\n",
    "S3.saveFile(key=key,path=path,bucket=bucket,\n",
    "            ExtraArgs = {**ExtraArgs.gzip, **ExtraArgs.publicRead})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test save gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gzip options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "@add_class_method(S3)\n",
    "def saveGz(cls, bucket, key, item, extraArgs = {**ExtraArgs.gzip}, path = '/tmp/tmp.gz',**kwargs):\n",
    "  data = json.dumps(item).encode()\n",
    "  compressedData = gzip.compress(data)\n",
    "  with open(path,'wb') as f:\n",
    "    f.write(compressedData)\n",
    "  S3.saveFile(key=key,path=path,bucket=bucket,\n",
    "          ExtraArgs = extraArgs)\n",
    "\n",
    "@add_class_method(S3)\n",
    "def loadGz(cls, bucket, key, path = '/tmp/test'):\n",
    "  S3.loadFile(key, bucket=bucket, path = path)\n",
    "  with open(path, 'rb') as f:\n",
    "    compressedData:bytes = f.read()\n",
    "  data = gzip.decompress(compressedData)\n",
    "  return json.loads(data.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': 'test'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = {'test':'test'}\n",
    "S3.saveGz(bucket=bucket,key = key, item=item)\n",
    "S3.loadGz(bucket=bucket,key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## presign class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# print(f'fileds are {p[\"fields\"]}')\n",
    "class Presign:\n",
    "  @staticmethod\n",
    "  def upload(signedUrlObject:dict, data:bytes, key:str):\n",
    "    p  = signedUrlObject\n",
    "    url = p['url']\n",
    "    fields = p['fields']\n",
    "    bio = BytesIO(data)\n",
    "    files = {'file': (key, bio)}\n",
    "    r = requests.post(url, data=fields, files= files)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### presign upload example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://pybz-test.s3-accelerate.amazonaws.com/', 'fields': {'Content-Type': 'image/png', 'key': 'test', 'AWSAccessKeyId': 'AKIAVX4Z5TKDSNNNULGB', 'policy': 'eyJleHBpcmF0aW9uIjogIjIwMjEtMDQtMDZUMDk6NTY6MDFaIiwgImNvbmRpdGlvbnMiOiBbWyJzdGFydHMtd2l0aCIsICIkQ29udGVudC1UeXBlIiwgIiJdLCB7ImJ1Y2tldCI6ICJweWJ6LXRlc3QifSwgeyJrZXkiOiAidGVzdCJ9XX0=', 'signature': 'pbtwCLfTa5+47nnbZLZv1gaWOcI='}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b''"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = S3.presignUpload(bucket, key='test', fields = {**ExtraArgs.png}, conditions = [[\"starts-with\", \"$Content-Type\", \"\"]])\n",
    "print(p)\n",
    "r = Presign.upload(p, b'hello', key = 'test')\n",
    "r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket pybz-test, has item hello\n"
     ]
    }
   ],
   "source": [
    "S3.loadFile('test', bucket=bucket, path = '/tmp/test')\n",
    "with open('/tmp/test', 'r') as f:\n",
    "  item = f.read()\n",
    "print(f'bucket {bucket}, has item {item}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "p = S3.presignUpload(bucket, key='test')\n",
    "url = p['url']\n",
    "fields = p['fields']\n",
    "bio = BytesIO(b'hello1')\n",
    "files = {'file': ('test1', bio)}\n",
    "r = requests.post(url, data=fields, files= files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@add_class_method(S3)\n",
    "def generalSave(cls, key, objectToSave:dict, bucket = '', \n",
    "                compressor=lambda x: zlib.compress(x), \n",
    "                encoder=lambda x: json.dumps(x).encode() ,**kwargs):\n",
    "  '''save a file to s3'''\n",
    "  s3 = cls.s3(**kwargs)\n",
    "  compressedData = compressor(encoder(objectToSave))\n",
    "  result = s3.put_object(Body=compressedData, Bucket=bucket, Key=key)\n",
    "  success = result['ResponseMetadata']['HTTPStatusCode'] ==  200\n",
    "  logging.info('data was saved to s3')\n",
    "  if not success: raise Error(success)\n",
    "  else: return True\n",
    "@add_class_method(S3)\n",
    "def generalLoad(cls, key, bucket = '',fileName = '/tmp/tempFile.bz', \n",
    "                decompressor=lambda x: zlib.decompress(x), \n",
    "                decoder=lambda x: json.loads(x.decode()), useUrl=False, **kwargs):\n",
    "  '''load file from s3'''\n",
    "  ### check object exist\n",
    "#   if not cls.exist(key, bucket, **kwargs):\n",
    "#     logging.info('object doesnt exist')\n",
    "#     return {}\n",
    "#   logging.info('object exists, loading')\n",
    "  ### download file\n",
    "  try:\n",
    "    s3 = cls.s3(**kwargs)\n",
    "#     s3.download_file(bucket,key, fileName ,useUrl=useUrl)\n",
    "    cls.loadFile(key=key, path=fileName, bucket=bucket, useUrl=useUrl)\n",
    "  except Exception as e:\n",
    "    print(f'downlaod failed {e}')\n",
    "  ### extract\n",
    "  with open (fileName , 'rb') as f:\n",
    "    allItemsByte = f.read()\n",
    "  if not allItemsByte: raise ValueError('all data does not exist in the database')\n",
    "  allItems = decoder(decompressor(allItemsByte))\n",
    "  return allItems\n",
    "  \n",
    "@add_class_method(S3)\n",
    "def saveZl(cls, key, objectToSave:dict, bucket = '', **kwargs):\n",
    "  '''save a file to s3'''\n",
    "  return cls.generalSave(key,objectToSave, bucket )\n",
    "@add_class_method(S3)\n",
    "def loadZl(cls, key, bucket = '',fileName = '/tmp/tempFile.bz', **kwargs):\n",
    "  '''load file from s3'''\n",
    "  return cls.generalLoad(key,bucket,fileName , **kwargs)\n",
    "  \n",
    "@add_class_method(S3)\n",
    "def savePklZl(cls, key, objectToSave:dict, bucket = '', **kwargs):\n",
    "  '''save a file to s3'''\n",
    "  return cls.generalSave(key,objectToSave, bucket, \n",
    "                         compressor=lambda x: zlib.compress(x), \n",
    "                         encoder=lambda x: pickle.dumps(x))\n",
    "  \n",
    "\n",
    "@add_class_method(S3)\n",
    "def loadPklZl(cls, key, bucket = '',fileName = '/tmp/tempFile.bz', **kwargs):\n",
    "  '''load file from s3'''\n",
    "  return cls.generalLoad(key,bucket,fileName, \n",
    "                         decompressor=lambda x: zlib.decompress(x),\n",
    "                         decoder = lambda x: pickle.loads(x), **kwargs)\n",
    "\n",
    "@add_class_method(S3)\n",
    "def saveRaw(cls, key, objectToSave, bucket = '', **kwargs):\n",
    "  '''save a file to s3'''\n",
    "  return cls.generalSave(key,objectToSave, bucket, \n",
    "                         compressor=lambda x: x, \n",
    "                         encoder=lambda x: json.dumps(x).encode())\n",
    "@add_class_method(S3)\n",
    "def loadRaw(cls, key, bucket = '',fileName = '/tmp/tempFile.bz', **kwargs):\n",
    "  '''load file from s3'''\n",
    "  return cls.generalLoad(key,bucket,fileName, \n",
    "                         decompressor=lambda x: x,\n",
    "                         decoder = lambda x: json.loads(x.decode()), **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "print(bucket)\n",
    "# sampleDict = {'hello':'world'}\n",
    "%time S3.save(key,sampleDict,bucket)\n",
    "%time S3.load(key,bucket, useUrl = True)\n",
    "%time S3.saveZl(key,sampleDict,bucket)\n",
    "%time S3.loadZl(key,bucket, useUrl = True)\n",
    "%time S3.savePklZl(key,sampleDict,bucket)\n",
    "%time r = S3.loadPklZl(key,bucket, useUrl = True)\n",
    "%time S3.saveRaw(key,sampleDict,bucket)\n",
    "%time r = S3.loadRaw(key,bucket, useUrl = True)\n",
    "%time url = S3.presign(key, bucket=bucket, checkExist=False)\n",
    "%time r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Requests:\n",
    "    '''\n",
    "      for uploading and downloading contents from url\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def getContentFromUrl( url):\n",
    "      result = requests.get(url)\n",
    "      if not result.ok:\n",
    "        print('error downloading')\n",
    "        return result.content\n",
    "      content = result.content\n",
    "      decompressedContent = bz2.decompress(content)\n",
    "      contentDict = json.loads(decompressedContent)\n",
    "      return contentDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "bucket = 'pybz-test'\n",
    "key = 'test.dict'\n",
    "sampleDict = {'test': 'bool'}\n",
    "USER = None\n",
    "PW = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'test':[1,2,3,4,5],'test2':[2,3,4,5,6]})\n",
    "S3.saveDataFrame(bucket,key,df)\n",
    "S3.loadDataFrame(bucket,key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# !bash build.sh\n",
    "# !nbdev_build_docs --mk_readme True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time S3.presign(key='allData', bucket = 'product-bucket-dev-manual')\n",
    "%time S3.presign(key='allData', bucket = 'product-bucket-dev-manual', checkExist=False)\n",
    "%timeit S3.exist(key='allData', bucket = 'product-bucket-dev-manual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import boto3\n",
    "from botocore.errorfactory import ClientError\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "try:\n",
    "    s3.head_object(Bucket='product-bucket-dev-manual', Key='allData')\n",
    "except ClientError:\n",
    "    # Not found\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from botocore.exceptions import ClientError\n",
    "@add_class_method(S3)\n",
    "def createBucket(cls, bucket:str, **kwargs):\n",
    "  s3 = cls.s3(**kwargs)\n",
    "  try:\n",
    "    s3.create_bucket(\n",
    "      Bucket=bucket,\n",
    "      CreateBucketConfiguration={'LocationConstraint':'ap-southeast-1'})\n",
    "  except ClientError as e:\n",
    "    print(e)\n",
    "  response = s3.put_bucket_accelerate_configuration(\n",
    "      Bucket=bucket ,\n",
    "      AccelerateConfiguration={\n",
    "          'Status': 'Enabled'\n",
    "      }\n",
    "  )\n",
    "  return response\n",
    "@add_class_method(S3)\n",
    "def deleteBucket(cls, bucket:str, **kwargs):\n",
    "  s3 = cls.s3(**kwargs)\n",
    "  response = s3.delete_bucket(\n",
    "    Bucket=bucket)\n",
    "  return response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python38"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
